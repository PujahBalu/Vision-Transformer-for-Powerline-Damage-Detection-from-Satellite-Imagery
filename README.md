# Vision Transformer for Powerline Damage Detection

## ğŸ” Overview
This project uses a Vision Transformer (ViT) model to classify satellite imagery and detect powerline infrastructure damage. It leverages the XView2 dataset and integrates with HuggingFace Transformers and Albumentations for preprocessing.

## ğŸ§  Objective
To automate the classification of grid infrastructure damage (e.g., from storms or fallen lines) using a Vision Transformer model.

## ğŸ“¦ Tech Stack
- Vision Transformer (ViT)
- HuggingFace Transformers
- Albumentations
- TensorBoard
- PyTorch

## ğŸ—‚ï¸ Project Structure
- `notebooks/` â€“ Jupyter notebooks for model development
- `src/` â€“ Python scripts for training, inference, preprocessing
- `images/` â€“ Sample output visualizations
- `data/` â€“ Placeholder for dataset

## ğŸ“ Dataset
XView2: Satellite Building Damage Assessment
[Kaggle Link](https://www.kaggle.com/competitions/xview2-building-damage-assessment)

## ğŸš€ Steps
1. Preprocess satellite imagery using Albumentations.
2. Fine-tune Vision Transformer for damage classification.
3. Visualize outputs and metrics in TensorBoard.
4. Optionally generate bounding boxes or masks.

## ğŸ“ˆ Results
Model performance is tracked using accuracy, precision, and confusion matrix.

## ğŸ“œ License
This project is licensed under the MIT License.

## ğŸ‘¤ Author
Pujah Balasubramaniam
